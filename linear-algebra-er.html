<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Basic Linear Algebra for Entity Resolution</title>
    <link rel="icon" type="image/x-icon" href="ico.png" />
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism.min.css"
      rel="stylesheet"
    />

    <style>
      body {
        font-family: "Inter", sans-serif;
        font-weight: 300;
        line-height: 1.6;
        color: #333;
        background-color: #fafafa;
        max-width: 800px;
        margin: 0 auto;
        padding: 80px 20px 40px;
        font-size: 16px;
      }
      h1,
      h2,
      h3 {
        font-weight: 400;
        margin: 1.5em 0 0.5em 0;
        letter-spacing: -0.5px;
      }
      h1 {
        font-size: 32px;
      }
      h2 {
        font-size: 24px;
      }
      h3 {
        font-size: 20px;
      }
      p {
        margin: 0 0 20px 0;
      }
      a {
        color: #0066cc;
        text-decoration: none;
      }
      a:hover {
        text-decoration: underline;
      }
      pre[class*="language-"] {
        border-radius: 5px;
        margin: 1em 0;
      }
      code[class*="language-"] {
        font-size: 12px;
      }
      .math-block {
        overflow-x: auto;
        margin: 1em 0;
        padding: 1em;
        background-color: #f5f5f5;
        border-radius: 5px;
      }
      .signature {
        margin-top: 20px;
        color: #666;
      }
      @media (max-width: 600px) {
        body {
          padding-top: 60px;
          font-size: 14px;
        }
        h1 {
          font-size: 28px;
        }
        h2 {
          font-size: 22px;
        }
        h3 {
          font-size: 18px;
        }
      }
    </style>
  </head>
  <body>
    <article>
      <h1>Basic Linear Algebra for Entity Resolution</h1>
      <h3>2024-08-13</h3>

      <p>
        Recently I've been diving back into mathematics. First I'm starting with
        linear algebra since the topic seems to dominate the industry these days
        with the proliferation of LLMs. Linear algebra is something I've used
        here and there in past implementations of computational workloads but I
        never really got a good look at the fundamentals of the discipline—even
        though I took linear algebra in my undergrad studies.
      </p>

      <p>
        In the effort of self-teaching myself the subject and mapping the
        lessons back to problems I have actually worked on I picked up a copy of
        <a href="https://a.co/d/2bzfOI9"
          >Linear Algebra and Optimization for Machine Learning by Charu C.
          Aggarwal</a
        >. The textbook is incredibly detailed and (unfortunately) proof-heavy
        however it does provide a good bit of detail into some of the
        fundamental cornerstones of linear algebra for pseudo-beginners like
        myself. Not all of the concepts stuck with me but I would highly
        recommend this book to anyone who wants to learn how linear algebra
        relates to current-state machine learning. I pretty much skipped over
        all of the proofs because I have never been able to grasp the handwavy
        nature of them. In undergrad I took three proof-heavy classes and my
        grades were C-, C-, and DNF respectively.
      </p>

      <p>
        This is effectively a book report on things I learned from Linear
        Algebra and Optimization for Machine Learning as they relate to the
        problem of entity resolution. Entity resolution is something I have
        worked on for quite a few years on and off in my career and I am
        intimately familiar with a lot of the issues engineers struggle with in
        designing and implementing entity resolution solutions.
      </p>

      <h2>The Entity Resolution Challenge</h2>

      <p>
        Entity resolution is a critical challenge in data integration and
        analytics. At its core, it's about determining when two data records are
        referring to the same real-world entity. It sounds straightforward, but
        it quickly becomes complex in practice.
      </p>

      <p>
        Let's consider a global e-commerce company trying to create a unified
        customer database from multiple sources:
      </p>

      <ul>
        <li>
          Customer A: "John Smith", "123 Main St, New York, NY",
          "johnsmith@email.com"
        </li>
        <li>
          Customer B: "J. Smith", "123 Main Street, NYC, New York",
          "john.smith@email.com"
        </li>
        <li>
          Customer C: "John Smith", "456 Elm St, Los Angeles, CA",
          "jsmith@email.com"
        </li>
      </ul>

      <p>
        Are these three separate customers, or could some (or all) of them be
        the same person? This is the essence of entity resolution.
      </p>

      <p>
        The challenge becomes exponentially complex as it scales to millions of
        records, each with dozens of attributes, potentially in different
        languages or formats. Traditional rule-based systems quickly become
        unwieldy and inefficient at this scale.
      </p>

      <p>
        This is where linear algebra comes in. By leveraging linear algebra, we
        can transform the entity resolution problem into a mathematical
        framework that's both powerful and computationally efficient.
      </p>

      <p>
        In this "guide", I'll explore how fundamental concepts from linear
        algebra can be applied to build a workable entity resolution system.
        I'll cover:
      </p>

      <ol>
        <li>
          Vectorization of entities: Transforming raw data into mathematical
          objects
        </li>
        <li>Dot products: A fundamental operation for comparing vectors</li>
        <li>Cosine similarity: A scale-invariant measure of similarity</li>
        <li>
          Eigenvectors and eigenvalues: Tools for dimensionality reduction and
          feature extraction
        </li>
        <li>
          Gradient descent: An optimization technique for fine-tuning our
          matching process
        </li>
      </ol>

      <p>
        For each topic, I'll delve into the mathematical foundations, explain
        their relevance to entity resolution, and discuss practical
        implementation considerations. By the end, you'll have a solid
        understanding of how these linear algebra concepts interplay to solve
        real-world entity resolution problems.
      </p>

      <h2>Vectorization of Entities</h2>

      <p>
        The first step in applying linear algebra to entity resolution is
        vectorization - the process of transforming raw data into numerical
        vectors. This transformation is crucial because it allows us to leverage
        the rich toolset of linear algebra and enables efficient computational
        techniques.
      </p>

      <p>Mathematically, it's possible to represent an entity e as a vector:</p>

      <p>e = [f₁, f₂, ..., fₙ]</p>

      <p>Where fᵢ represents the i-th feature of the entity.</p>

      <p>
        The challenge lies in choosing these features and transforming diverse
        data types into numerical values. Here are some strategies that can be
        employed:
      </p>

      <ol>
        <li>
          <strong>Numerical data</strong>: For features like age or income, we
          can use the values directly. However, it's often beneficial to
          normalize these values to a common scale (e.g., 0 to 1) to prevent
          features with larger ranges from dominating the calculations
        </li>
        <li>
          <strong>Categorical data</strong>: For features like industry sector
          or geographical region, we can use techniques like one-hot encoding or
          embeddings. For instance, in one-hot encoding, a 'country' feature
          with options 'USA', 'Canada', and 'Mexico' might be represented as [1,
          0, 0], [0, 1, 0], and [0, 0, 1] respectively. This preserves
          categorical distinctions without imposing an arbitrary numerical order
        </li>
        <li>
          <strong>Text data</strong>: For names or addresses, there are several
          options:
          <ul>
            <li>
              Character-level encoding: Each character position becomes a
              feature
            </li>
            <li>
              N-gram encoding: Create features for all n-letter sequences in the
              text
            </li>
            <li>
              Hash encoding: Use a hash function to map the text to a fixed-size
              vector
            </li>
          </ul>
        </li>
        <li>
          <strong>Datetime data</strong>: Possibly separate this into multiple
          numerical features (year, month, day) or calculate the time difference
          from a reference date
        </li>
      </ol>

      <p>
        Here's a simple Python implementation to illustrate entity
        vectorization:
      </p>

      <pre><code class="language-python">
class Entity:
    def __init__(self, name, address, age, email):
        self.name = name
        self.address = address
        self.age = age
        self.email = email

    def to_vector(self):
        def string_to_number(s):
            return sum(ord(c) for c in s)

        return [
            string_to_number(self.name),
            string_to_number(self.address),
            self.age,
            string_to_number(self.email)
        ]

person1 = Entity("John Smith", "123 Main St, New York, NY", 30, "johnsmith@email.com")
person2 = Entity("J. Smith", "123 Main Street, NYC, New York", 31, "john.smith@email.com")

print(person1.to_vector())
print(person2.to_vector())
      </code></pre>

      <p>
        When dealing with multiple entities, organize the vectors into a matrix.
        If there are e m entities, each with n features, it creates an m × n
        matrix E:
      </p>

      <p>E = [e₁ᵀ; e₂ᵀ; ...; eₘᵀ]</p>

      <p>Where eᵢᵀ is the transpose of the i-th entity vector.</p>

      <p>
        This matrix representation is powerful because it allows us to perform
        operations on all entities simultaneously, greatly improving
        computational efficiency.
      </p>

      <h2>Dot Products</h2>

      <p>
        With the entities represented as vectors, there needs to be a way to
        compare them. The dot product serves as the fundamental operation for
        this comparison, underpinning many similarity measures used in entity
        resolution.
      </p>

      <p>The dot product of two vectors a and b is defined as:</p>

      <p>a · b = Σ(aᵢ * bᵢ) = a₁b₁ + a₂b₂ + ... + aₙbₙ</p>

      <p>
        Geometrically, the dot product is related to the cosine of the angle
        between the vectors:
      </p>

      <p>a · b = ||a|| ||b|| cos(θ)</p>

      <p>
        Where ||a|| and ||b|| are the magnitudes of vectors a and b, and θ is
        the angle between them.
      </p>

      <p>Here's a simple Python implementation of the dot product:</p>

      <pre><code class="language-python">
def dot_product(vector1, vector2):
    if len(vector1) != len(vector2):
        raise ValueError("Vectors must have the same length")
    return sum(a * b for a, b in zip(vector1, vector2))

entity1 = [1000, 2000, 30, 5551234]
entity2 = [1100, 2100, 31, 5555678]

similarity = dot_product(entity1, entity2)
print(f"Dot product similarity: {similarity}")
      </code></pre>

      <p>
        In the context of entity resolution, the dot product provides a measure
        of similarity between two entity vectors. A larger dot product generally
        indicates greater similarity. However, interpreting dot products
        requires careful consideration:
      </p>

      <ol>
        <li>
          <strong>Magnitude sensitivity</strong>: The dot product is sensitive
          to the magnitudes of the vectors. This means that two very similar
          entities with large feature values will have a larger dot product than
          two very similar entities with smaller feature values
        </li>
        <li>
          <strong>Positive values</strong>: When working with non-negative
          feature values (as is common in entity resolution), the dot product
          will always be non-negative. A value of 0 indicates orthogonal
          (completely dissimilar) vectors
        </li>
        <li>
          <strong>Scale consideration</strong>: Features with larger scales will
          have a more significant impact on the dot product. This can be
          problematic if some features are arbitrarily larger than others
        </li>
        <li>
          <strong>Sparse vector efficiency</strong>: For sparse vectors (common
          in text-based features), dot product calculations can be optimized by
          only considering non-zero elements
        </li>
      </ol>

      <h2>Cosine Similarity</h2>

      <p>
        Cosine similarity addresses many of the limitations of raw dot products
        and provides a more nuanced way to compare entity vectors.
      </p>

      <p>Cosine similarity is defined as:</p>

      <p>cos_sim(a, b) = (a · b) / (||a|| ||b||)</p>

      <p>This simple transformation yields several powerful properties:</p>

      <ol>
        <li>
          <strong>Scale invariance</strong>: Cosine similarity is independent of
          vector magnitudes. This means that two entities with proportionally
          similar features will have high similarity, regardless of the absolute
          values of their features
        </li>
        <li>
          <strong>Bounded output</strong>: For non-negative features (common in
          entity resolution), cosine similarity is bounded between 0 and 1. This
          makes it easier to set universal thresholds for matching
        </li>
        <li>
          <strong>Intuitive interpretation</strong>: A cosine similarity of 1
          means the vectors are pointing in the same direction (i.e., the
          entities are identical in terms of their feature proportions), while 0
          means they are orthogonal (completely dissimilar)
        </li>
      </ol>

      <p>Here's a Python implementation of cosine similarity:</p>

      <pre><code class="language-python">
import math

def magnitude(vector):
    return math.sqrt(sum(x**2 for x in vector))

def dot_product(vector1, vector2):
    return sum(a * b for a, b in zip(vector1, vector2))

def cosine_similarity(vector1, vector2):
    dot_prod = dot_product(vector1, vector2)
    mag1 = magnitude(vector1)
    mag2 = magnitude(vector2)
    
    if mag1 == 0 or mag2 == 0:
        return 0  # Handle zero vectors
    
    return dot_prod / (mag1 * mag2)

entity1 = [0.5, 0.8, 0.3, 0.7]
entity2 = [0.4, 0.9, 0.2, 0.6]

similarity = cosine_similarity(entity1, entity2)
print(f"Cosine similarity: {similarity}")
      </code></pre>

      <h2>Eigenvectors and Eigenvalues</h2>

      <p>
        As the entity resolution system scales to handle larger and more complex
        datasets, it starts to face new challenges:
      </p>

      <ol>
        <li>
          High-dimensional feature spaces can lead to computational inefficiency
        </li>
        <li>
          Noise and redundancy in the features can obscure true entity
          relationships
        </li>
        <li>
          Visualizing and interpreting high-dimensional entity relationships
          becomes difficult
        </li>
      </ol>

      <p>
        Eigenvectors and eigenvalues provide powerful tools to address these
        challenges, allowing us to uncover the underlying structure of the data
        and reduce its dimensionality while preserving essential information.
      </p>

      <p>
        In linear algebra, an eigenvector of a square matrix A is a non-zero
        vector v that, when multiplied by A, yields a scalar multiple of itself.
        This scalar is called the eigenvalue corresponding to that eigenvector.
        Mathematically:
      </p>

      <p>Av = λv</p>

      <p>Where:</p>
      <ul>
        <li>A is a square matrix</li>
        <li>v is an eigenvector of A</li>
        <li>λ (lambda) is the eigenvalue corresponding to v</li>
      </ul>

      <p>
        Intuitively, eigenvectors represent directions in which a linear
        transformation acts by simple scaling, and eigenvalues represent the
        amount of scaling.
      </p>

      <p>
        Here's a simple implementation of the power iteration method to find the
        dominant eigenvector and eigenvalue:
      </p>

      <pre><code class="language-python">
import random
import math

def dot_product(v1, v2):
    return sum(x*y for x, y in zip(v1, v2))

def matrix_vector_multiply(matrix, vector):
    return [dot_product(row, vector) for row in matrix]

def normalize(vector):
    magnitude = math.sqrt(sum(x**2 for x in vector))
    return [x/magnitude for x in vector]

def power_iteration(matrix, num_iterations=100):
    n = len(matrix)
    vector = [random.random() for _ in range(n)]
    vector = normalize(vector)
    
    for _ in range(num_iterations):
        new_vector = matrix_vector_multiply(matrix, vector)
        new_vector = normalize(new_vector)
        vector = new_vector
    
    eigenvalue = dot_product(matrix_vector_multiply(matrix, vector), vector)
    return vector, eigenvalue

matrix = [
    [4, -1, 1],
    [-1, 3, -2],
    [1, -2, 3]
]

eigenvector, eigenvalue = power_iteration(matrix)
print(f"Dominant eigenvector: {eigenvector}")
print(f"Corresponding eigenvalue: {eigenvalue}")
      </code></pre>

      <p>
        In entity resolution, eigenvectors and eigenvalues are particularly
        useful for dimensionality reduction and feature extraction. Here are key
        applications:
      </p>

      <ol>
        <li>
          <strong>Principal Component Analysis (PCA)</strong>: PCA uses the
          eigenvectors of the data covariance matrix to identify the principal
          components – directions in which the data varies the most. By
          projecting the entity vectors onto the top few principal components,
          it can dramatically reduce dimensionality while preserving most of the
          variance in the data
        </li>
        <li>
          <strong>Spectral Clustering</strong>: This technique uses eigenvectors
          of the similarity matrix (or its Laplacian) to perform clustering.
          It's particularly effective for entity resolution when dealing with
          complex, non-linear relationships between entities
        </li>
        <li>
          <strong>Latent Semantic Analysis (LSA)</strong>: In text-heavy entity
          resolution tasks, LSA uses the singular value decomposition (closely
          related to eigenvectors and eigenvalues) to identify latent concepts
          in the text data
        </li>
      </ol>

      <p>
        Here's a simple example of how one might apply PCA to the entity
        vectors:
      </p>

      <pre><code class="language-python">
def calculate_mean_vector(vectors):
    n = len(vectors[0])
    return [sum(v[i] for v in vectors) / len(vectors) for i in range(n)]

def subtract_mean(vectors, mean_vector):
    return [[v[i] - mean_vector[i] for i in range(len(v))] for v in vectors]

def calculate_covariance_matrix(vectors):
    n = len(vectors[0])
    cov_matrix = [[0] * n for _ in range(n)]
    for i in range(n):
        for j in range(n):
            cov_matrix[i][j] = sum(v[i] * v[j] for v in vectors) / (len(vectors) - 1)
    return cov_matrix

def pca(vectors, num_components):
    mean_vector = calculate_mean_vector(vectors)
    centered_vectors = subtract_mean(vectors, mean_vector)
    cov_matrix = calculate_covariance_matrix(centered_vectors)
    
    eigenvectors = []
    eigenvalues = []
    for _ in range(num_components):
        eigenvector, eigenvalue = power_iteration(cov_matrix)
        eigenvectors.append(eigenvector)
        eigenvalues.append(eigenvalue)
    
    return eigenvectors, eigenvalues

entities = [
    [1000, 2000, 30, 5551234],
    [1100, 2100, 31, 5555678],
    [500, 1000, 25, 5559876]
]

principal_components, variances = pca(entities, 2)
print("Principal Components:")
for i, pc in enumerate(principal_components):
    print(f"PC{i+1}: {pc}")
print("Variances:", variances)

# Project entities onto principal components
projected_entities = [
    [dot_product(entity, pc) for pc in principal_components]
    for entity in entities
]
print("Projected Entities:")
for entity in projected_entities:
    print(entity)
      </code></pre>

      <h2>Gradient Descent</h2>

      <p>
        The entity resolution system built using various linear algebra
        techniques involves several parameters and decisions that affect its
        performance. Gradient descent provides a powerful, general-purpose
        optimization technique that can help fine-tune these aspects of the
        system, improving its accuracy and adaptability.
      </p>

      <p>
        Gradient descent is an iterative optimization algorithm used to find the
        minimum of a function. The basic idea is to iteratively move in the
        direction of steepest descent (negative gradient) to reach the minimum
        of a function.
      </p>

      <p>Mathematically, for a function f(x), the update step is:</p>

      <p>x = x - α * ∇f(x)</p>

      <p>Where:</p>
      <ul>
        <li>x is the parameter we're optimizing</li>
        <li>α (alpha) is the learning rate</li>
        <li>∇f(x) is the gradient of f with respect to x</li>
      </ul>

      <p>
        Let's implement a simple gradient descent algorithm to optimize a
        weighted similarity function for entity matching:
      </p>

      <pre><code class="language-python">
import random
import math

def dot_product(v1, v2):
    return sum(x*y for x, y in zip(v1, v2))

def weighted_similarity(entity1, entity2, weights):
    return sum(w * (1 - abs(e1 - e2) / (abs(e1) + abs(e2) + 1e-8))
               for w, e1, e2 in zip(weights, entity1, entity2))

def gradient(entity1, entity2, weights):
    return [-(1 - abs(e1 - e2) / (abs(e1) + abs(e2) + 1e-8))
            for e1, e2 in zip(entity1, entity2)]

def loss_function(entities, matching_pairs, non_matching_pairs, weights):
    loss = 0
    for e1, e2 in matching_pairs:
        loss -= weighted_similarity(entities[e1], entities[e2], weights)
    for e1, e2 in non_matching_pairs:
        loss += weighted_similarity(entities[e1], entities[e2], weights)
    return loss

def gradient_descent(entities, matching_pairs, non_matching_pairs, learning_rate=0.01, num_iterations=1000):
    num_features = len(entities[0])
    weights = [random.random() for _ in range(num_features)]
    
    for _ in range(num_iterations):
        grad = [0] * num_features
        for e1, e2 in matching_pairs:
            g = gradient(entities[e1], entities[e2], weights)
            grad = [grad[i] - g[i] for i in range(num_features)]
        for e1, e2 in non_matching_pairs:
            g = gradient(entities[e1], entities[e2], weights)
            grad = [grad[i] + g[i] for i in range(num_features)]
        
        weights = [w - learning_rate * g for w, g in zip(weights, grad)]
        weights = [max(0, w) for w in weights]  # Ensure non-negative weights
    
    return weights

entities = [
    [1000, 2000, 30, 5551234],  # Entity 0
    [1100, 2100, 31, 5555678],  # Entity 1
    [500, 1000, 25, 5559876],   # Entity 2
    [2000, 3000, 40, 5551111]   # Entity 3
]

matching_pairs = [(0, 1)]  # Entities 0 and 1 are a match
non_matching_pairs = [(0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]

optimized_weights = gradient_descent(entities, matching_pairs, non_matching_pairs)
print("Optimized weights:", optimized_weights)

# Test the optimized weights
for i in range(len(entities)):
    for j in range(i+1, len(entities)):
        sim = weighted_similarity(entities[i], entities[j], optimized_weights)
        print(f"Similarity between Entity {i} and Entity {j}: {sim:.4f}")
      </code></pre>

      <p>
        Gradient descent can be applied to entity resolution in several ways:
      </p>

      <ol>
        <li>
          <strong>Optimizing Similarity Measures</strong>: As demonstrated in
          the example, it's possible to use gradient descent to learn optimal
          weights for different features in the similarity function
        </li>
        <li>
          <strong>Training Machine Learning Models</strong>: More complex entity
          resolution systems often use machine learning models which are
          typically trained using variants of gradient descent
        </li>
        <li>
          <strong>Hyperparameter Tuning</strong>: Gradient descent can be used
          to optimize hyperparameters in the entity resolution pipeline, such as
          thresholds for matching or parameters in dimensionality reduction
          techniques
        </li>
      </ol>

      <div class="signature">
        <p>the end</p>
      </div>
    </article>

    <a href="index.html" class="back-button">Back to Home</a>

    <!-- Prism JS for code highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- MathJax for rendering mathematical equations -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
          displayMath: [
            ["$$", "$$"],
            ["\\[", "\\]"],
          ],
          processEscapes: true,
        },
        options: {
          skipHtmlTags: ["script", "noscript", "style", "textarea", "pre"],
        },
      };
    </script>
  </body>
</html>
